{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bbb7a81-e7d2-4fde-9da8-3b682c01957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 14.83668327331543\n",
      "Epoch 10, Loss: 0.014167988672852516\n",
      "Epoch 20, Loss: 0.00019317257101647556\n",
      "Epoch 30, Loss: 0.0008287914679385722\n",
      "Epoch 40, Loss: 0.0007234986405819654\n",
      "Epoch 50, Loss: 0.0006213100859895349\n",
      "Epoch 60, Loss: 0.0005637628491967916\n",
      "Epoch 70, Loss: 0.00048324663657695055\n",
      "Epoch 80, Loss: 0.0004456360184121877\n",
      "Epoch 90, Loss: 7.479487976524979e-05\n",
      "tensor([[10.0237]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Data preparation\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define a simple linear model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    print(model(torch.tensor([[5.0]])))  # Expected: ~10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94acc0-54e4-4190-8391-392403779cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. PyTorch linear regression example.\n",
    "2. Data: `X` and `y` tensors for `y = 2X`.\n",
    "3. `TensorDataset` combines data, `DataLoader` batches it.\n",
    "4. Model: `LinearRegressionModel` with `nn.Linear(1, 1)`.\n",
    "5. Loss: `MSELoss`, Optimizer: `SGD` with lr=0.01.\n",
    "6. Training: 100 epochs, batches of 2, backpropagation.\n",
    "7. Prints loss every 10 epochs.\n",
    "8. Tests with `X=5.0`, expects ~10.0 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f90ead61-0fcd-495e-bc48-94ec84f13b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3429 - loss: 1.0947\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3976 - loss: 1.1132 \n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4458 - loss: 1.0979 \n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4427 - loss: 1.0761 \n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3671 - loss: 1.0770 \n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5241 - loss: 1.0354 \n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4232 - loss: 1.0575 \n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4935 - loss: 1.0469 \n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4785 - loss: 1.0530 \n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4414 - loss: 1.0477 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22ba1e24ec0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. TensorFlow Example – Building a Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a simple feedforward neural network\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data\n",
    "data = tf.random.normal((100, 4))\n",
    "labels = tf.random.uniform((100,), maxval=3, dtype=tf.int32)\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, labels, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a957f18-8258-4344-adfb-cd990edd8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. TensorFlow/Keras for feedforward neural network.\n",
    "2. `Sequential` stack with three `Dense` layers.\n",
    "3. Input layer: 16 neurons, ReLU activation, input shape (4,).\n",
    "4. Hidden layer: 8 neurons, ReLU activation.\n",
    "5. Output layer: 3 neurons, softmax activation.\n",
    "6. `Adam` optimizer and `sparse_categorical_crossentropy` loss.\n",
    "7. Random data: 100 samples with 4 features, labels in [0, 2].\n",
    "8. `model.fit()` trains for 10 epochs with batch size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c902d119-8baa-40dd-9b08-5820fa0d4afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8759 - loss: 0.4363\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9649 - loss: 0.1164\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0769\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0579\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22ba2276b40>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Keras Example – Image Classification with MNIST\n",
    "tf.keras.datasets.mnist.load_data()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84105186-a8bf-472f-866a-e18f87c6368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. MNIST dataset: Handwritten digits dataset with 28x28 grayscale images.\n",
    "2. Data loading: `tf.keras.datasets.mnist.load_data()` loads training and test sets.\n",
    "3. Normalization: Data is scaled to [0,1] by dividing by 255.\n",
    "4. Model architecture: `Sequential` stack with `Flatten`, `Dense(128, relu)`, and `Dense(10, softmax)` layers.\n",
    "5. Flatten layer: Converts 28x28 images into 1D vectors of 784 pixels.\n",
    "6. Loss and optimizer: `Adam` optimizer with `sparse_categorical_crossentropy` loss.\n",
    "7. Training: `model.fit()` trains the model for 5 epochs on training data.\n",
    "8. Output: Trained model predicts digit classes with accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9aa19c6-af88-4b7c-8c7d-1ecad107fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight: [1.6226422]\n",
      "Initial bias: 0.0\n",
      "Gradients - weight: [0.]\n",
      "Gradients - bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define model parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "w = jax.random.normal(key, (1,))\n",
    "b = 0.0\n",
    "\n",
    "def model(x):\n",
    "    return w * x + b\n",
    "\n",
    "# Loss function and gradient calculation\n",
    "loss = lambda w, b, x, y: jnp.mean((model(x) - y)**2)\n",
    "grad_fn = jax.grad(loss, argnums=(0, 1))\n",
    "\n",
    "# Example data\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = jnp.array([2.0, 4.0, 6.0])\n",
    "\n",
    "# Compute gradients\n",
    "grad_w, grad_b = grad_fn(w, b, x, y)\n",
    "\n",
    "# Print outputs\n",
    "print(\"Initial weight:\", w)\n",
    "print(\"Initial bias:\", b)\n",
    "print(\"Gradients - weight:\", grad_w)\n",
    "print(\"Gradients - bias:\", grad_b)\n",
    "\n",
    "# Explanation:\n",
    "# JAX provides automatic differentiation and prints the initial weight, bias, and computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb8c09-16c0-47ba-81fd-b105f6203b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. JAX linear regression example.\n",
    "2. `jax` for differentiation, `jax.numpy` like NumPy.\n",
    "3. Parameters: `w` (random normal) and `b` (0).\n",
    "4. Model: simple `y = wx + b`.\n",
    "5. Loss: MSE using `jax.numpy.mean`.\n",
    "6. `jax.grad` calculates gradients for `w` and `b`.\n",
    "7. Data: `x` and `y` arrays for `y = 2x`.\n",
    "8. Prints initial parameters and computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fac146c4-50ff-4b88-96ae-d5b7216098d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize and train a Logistic Regression model\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model and print accuracy\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e960b-4170-4065-af8a-c4771bac202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. `from sklearn.datasets import load_iris` – Loads the Iris dataset with 150 samples and 4 features.  \n",
    "2. `from sklearn.linear_model import LogisticRegression` – Imports logistic regression for classification tasks.  \n",
    "3. `from sklearn.model_selection import train_test_split` – Imports a utility to split data into train and test sets.  \n",
    "4. `X, y = load_iris(return_X_y=True)` – Loads the dataset into `X` (features) and `y` (labels).  \n",
    "5. `train_test_split(X, y, test_size=0.2)` – Splits the dataset, reserving 20% for testing.  \n",
    "6. `clf = LogisticRegression().fit(X_train, y_train)` – Creates and trains the logistic regression model.  \n",
    "7. `clf.score(X_test, y_test)` – Evaluates the model on the test set.  \n",
    "8. Prints the accuracy, showing the percentage of correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ebf3020-babd-478f-a5f8-d787506ac30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Graph:\n",
      "graph main_graph (\n",
      "  %onnx::Gemm_0[FLOAT, 1x2]\n",
      ") initializers (\n",
      "  %weight[FLOAT, 2x2]\n",
      "  %bias[FLOAT, 2]\n",
      ") {\n",
      "  %3 = Gemm[alpha = 1, beta = 1, transB = 1](%onnx::Gemm_0, %weight, %bias)\n",
      "  return %3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Define a simple PyTorch model\n",
    "model = torch.nn.Linear(2, 2)\n",
    "dummy_input = torch.randn(1, 2)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\n",
    "\n",
    "# Load the exported ONNX model and print the output\n",
    "import onnx\n",
    "onnx_model = onnx.load('model.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(\"ONNX Model Graph:\")\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b84c8d30-dadf-4b80-a586-1ba881a5a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explanation:\n",
    "# 1. Imports PyTorch and ONNX libraries.\n",
    "# 2. Defines a simple linear model with 2 input and 2 output features.\n",
    "# 3. Creates a dummy input tensor of size (1, 2) for the export process.\n",
    "# 4. Exports the model to ONNX format using `torch.onnx.export`.\n",
    "# 5. Loads the saved ONNX model and checks its integrity.\n",
    "# 6. Prints the structure of the ONNX model graph.\n",
    "# ONNX allows interoperability between different deep learning frameworks.\n",
    "# Explanation of Output:\n",
    "# The output shows the computational graph of the exported ONNX model.\n",
    "# It includes:\n",
    "# - Input Tensor: The dummy input provided.\n",
    "# - Operations: Linear transformation (weights, bias) from the PyTorch model.\n",
    "# - Output Tensor: The result after applying the linear layer.\n",
    "# - Nodes and Edges: Each node represents an operation, and edges represent data flow.\n",
    "# - Metadata: Shape, data types, and operations used.\n",
    "# This structure allows the model to be easily imported into other frameworks or optimized for different runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f17106a7-d1a4-4449-89be-a0b15093ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\anaconda3\\Lib\\site-packages\\fastai\\vision\\learner.py:303: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to C:\\Users\\shash/.cache\\torch\\hub\\checkpoints\\resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:12<00:00, 7.23MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.165474</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>15:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.078476</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>21:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "# Corrected label function to handle string paths directly\n",
    "def label_func(fname):\n",
    "    return fname[0].isupper()\n",
    "\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=label_func, item_tfms=Resize(224))\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cc04b-7bfd-4f8a-b80d-29ea33714b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# - Fastai is a high-level library built on PyTorch, simplifying deep learning workflows.\n",
    "# - The dataset is automatically downloaded and extracted.\n",
    "# - DataLoaders handle loading, transforming, and batching images.\n",
    "# - `cnn_learner` provides an easy interface to fine-tune pre-trained models.\n",
    "# - Fine-tuning allows leveraging pre-trained weights while adapting to the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d178b9d-e746-41c5-bde0-2335db339ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 10. Gradient Boosting Libraries Example – XGBoost for Classification\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# Explanation:\n",
    "# XGBoost is a powerful library for gradient boosting, popular for structured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1398c-f55c-4389-b91b-c8d16ef70557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an XGBoost classifier model\n",
    "# XGBClassifier is a gradient boosting algorithm designed for structured/tabular data.\n",
    "# Train the model using training data (X_train, y_train)\n",
    "# `fit` method trains the model by optimizing the loss function through boosting iterations.\n",
    "# Evaluate the trained model on the test dataset (X_test, y_test)\n",
    "# `score` returns the mean accuracy on the given test data and labels.\n",
    "# Explanation:\n",
    "# - XGBoost is a powerful and efficient implementation of gradient boosting.\n",
    "# - The library is highly optimized for speed and performance.\n",
    "# - It handles missing values, offers built-in cross-validation, and parallel processing.\n",
    "# - `XGBClassifier` is specifically used for classification tasks, making it popular in machine learning competitions and real-world applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
